{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cbe40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../data/preprocessed_train_data/audio/5a03d20a7ecfc50001be0a7a_q1_generic.pt')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"../data/preprocessed_train_data\")\n",
    "audio_dir = data_dir / \"audio\"\n",
    "\n",
    "audio_path = sorted(audio_dir.glob(\"*.pt\"))[0]\n",
    "audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb9c5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1969, -0.0129, -0.0882,  ...,  0.1390,  0.2094, -0.1379],\n",
       "         [ 0.0950,  0.0111, -0.0779,  ...,  0.0576,  0.1827, -0.2337],\n",
       "         [ 0.0894,  0.0031, -0.0777,  ...,  0.0161,  0.2006, -0.1645],\n",
       "         ...,\n",
       "         [-0.1300, -0.0172,  0.1780,  ...,  0.1942,  0.2564, -0.2170],\n",
       "         [-0.0504, -0.0843,  0.2044,  ...,  0.3398,  0.2339, -0.1538],\n",
       "         [ 0.0152, -0.0241,  0.1738,  ...,  0.4174,  0.0609, -0.0242]]),\n",
       " torch.Size([20679, 768]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "audio_tensor = torch.load(audio_path)\n",
    "audio_tensor, audio_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd5aceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generally my greatest strength as an employee is my empathy. I'm very good at communicating with others because I'm very good at understanding where they're coming from. Even when they're upset, I can usually deal with people without getting to upset myself. And on the other side of that, my greatest weakness as an employee is that I'm kind of shy. I'm good at talking to people in a little community and to people that I don't really like to be very social like with my coworkers. But I'm good at dealing with customers.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(\"temp.wav\", temperature=0)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4165d950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/solan/repos/airi-research/.venv/lib/python3.12/site-packages/transformers/configuration_utils.py:312: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForXVector were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'feature_extractor.bias', 'feature_extractor.weight', 'objective.weight', 'projector.bias', 'projector.weight', 'tdnn.0.kernel.bias', 'tdnn.0.kernel.weight', 'tdnn.1.kernel.bias', 'tdnn.1.kernel.weight', 'tdnn.2.kernel.bias', 'tdnn.2.kernel.weight', 'tdnn.3.kernel.bias', 'tdnn.3.kernel.weight', 'tdnn.4.kernel.bias', 'tdnn.4.kernel.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForXVector(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2SdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projector): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (tdnn): ModuleList(\n",
       "    (0): TDNNLayer(\n",
       "      (kernel): Linear(in_features=2560, out_features=512, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (1-2): 2 x TDNNLayer(\n",
       "      (kernel): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (3): TDNNLayer(\n",
       "      (kernel): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (4): TDNNLayer(\n",
       "      (kernel): Linear(in_features=512, out_features=1500, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (feature_extractor): Linear(in_features=3000, out_features=512, bias=True)\n",
       "  (classifier): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (objective): AMSoftmaxLoss(\n",
       "    (loss): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForXVector, Wav2Vec2Processor\n",
    "\n",
    "model_id = \"facebook/wav2vec2-base\"\n",
    "model = Wav2Vec2ForXVector.from_pretrained(model_id, use_safetensors=True).to(\n",
    "    \"cpu\"  # args.audio_model_device\n",
    ")\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c01f03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2367288])\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "waveform, sr = torchaudio.load(\"temp.wav\")\n",
    "# waveform = waveform.to(\"cuda\")\n",
    "print(waveform.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68480e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2367288])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal = processor(waveform, sampling_rate=16000, return_tensors=\"pt\")\n",
    "signal.input_values.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e06c3c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.getsizeof(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60117208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1deabe72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07e8541c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a2e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers.logging.set_verbosity_error()\n",
    "\n",
    "# model_id = \"facebook/wav2vec2-base\"\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\n",
    "#     model_id, use_safetensors=True\n",
    "# ).to(args.audio_model_device)\n",
    "# processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "# val_file_paths = sorted(VAL_DIR_PATH.glob(\"*.mp4\"))\n",
    "\n",
    "# emoti_eff: EmotiEffLibRecognizerBase = EmotiEffLibRecognizer(\n",
    "#     model_name=\"enet_b0_8_best_vgaf\",\n",
    "#     device=args.video_model_device,\n",
    "#     engine=\"torch\",\n",
    "# )\n",
    "\n",
    "\n",
    "# features: list[np.ndarray] = []\n",
    "# batch_size = 32\n",
    "# for i in range(0, len(cropped_frames), batch_size):\n",
    "#     batch = cropped_frames[i : i + batch_size]\n",
    "#     batch_features = emoti_eff.extract_features(batch)\n",
    "#     features.extend(batch_features)\n",
    "\n",
    "# # boost performance of conversions list[np.ndarray] -> np.ndarray\n",
    "# features = np.array(features)\n",
    "# video_features: Float[torch.Tensor, \"frames features\"] = (  # noqa: F722\n",
    "#     torch.from_numpy(features)\n",
    "# )\n",
    "# torch.save(\n",
    "#     obj=video_features,\n",
    "#     f=preprocessed_dir_path / \"video\" / f\"{sample_path.stem}.pt\",\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
